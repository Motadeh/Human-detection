{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqZ4BcjaTnX7sqoOT4+59S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Motadeh/Human-detection/blob/main/Machine_learning_and_Computer_vison_Human_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Human detection Project\n",
        "\n",
        "This project work has two sections. The first section shows human detection using my own HOG feature extraction function and the second section shows human detection using high level off-shelf functions.\n",
        "\n",
        "Based on experiment carried out, the cell size of 3*3 produces a better result on both implementations"
      ],
      "metadata": {
        "id": "aHVsqIvf7IzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human Detection with built HOG feature extraction function\n",
        "\n",
        "In this section, HOG feature extraction function was created from scratch"
      ],
      "metadata": {
        "id": "j8UhBeOeyeDK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0I9QtMiemSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8fb66b-f3d6-4618-bcbd-f2a0b9ddf425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "ny3-ldddE9ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import scipy.signal as sig\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import io, color"
      ],
      "metadata": {
        "id": "CYv7QRZ_5Fir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset preparation\n",
        "\n",
        "This section handles creating both negative and positive patches."
      ],
      "metadata": {
        "id": "Ay-kg5wdzNUZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive patches are created from Image set containing just humans and converted to 2-D 128*64 image patches."
      ],
      "metadata": {
        "id": "jrw0peE9zY1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = glob.glob('/content/drive/MyDrive/Colab Notebooks/pedestrians128x64/*.ppm')"
      ],
      "metadata": {
        "id": "ViNMntrzpsRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = glob.glob('/content/drive/MyDrive/Colab Notebooks/pedestrians128x64/*.ppm')\n",
        "\n",
        "positive = [] \n",
        "for item in path:\n",
        "  positive_image = cv2.imread(item)\n",
        "  postive_test = np.array(positive_image)\n",
        "  to_gray2 = cv2.cvtColor(postive_test, cv2.COLOR_BGR2GRAY)\n",
        "  positive.append(to_gray2)\n",
        "  \n",
        "\n",
        "numpy_positive = np.array(positive)\n",
        "np.save(\"positive.npy\", numpy_positive)\n",
        "positive_patches = numpy_positive"
      ],
      "metadata": {
        "id": "wS4n-FrpsFAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative patches for were created from skimage exisitng image data and are reshaped and resized to into 2-D 128*64 image"
      ],
      "metadata": {
        "id": "WCtNGl1A0Laf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import data, color\n",
        "\n",
        "imgs_to_use = ['immunohistochemistry', 'hubble_deep_field']\n",
        "images = [color.rgb2gray(getattr(data, name)()) for name in imgs_to_use]"
      ],
      "metadata": {
        "id": "FdyR4kuA2L49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.image import PatchExtractor\n",
        "from skimage import transform\n",
        "import numpy as np\n",
        "\n",
        "def extract_patches(img, N, scale=1, patch_size=positive_patches[0].shape):\n",
        "    extracted_patch_size = tuple((scale * np.array(patch_size)).astype(int))\n",
        "    extractor = PatchExtractor(patch_size=extracted_patch_size,\n",
        "                               max_patches=N, random_state=0)\n",
        "    patches = extractor.transform(img[np.newaxis])\n",
        "    if scale != 1:\n",
        "        patches = np.array([transform.resize(patch, patch_size)\n",
        "                            for patch in patches])\n",
        "    return patches\n",
        "\n",
        "negative_patches = np.vstack([extract_patches(im, 1000, scale)\n",
        "                              for im in images for scale in [0.5, 1.0, 2.0]])\n",
        "negative_patches.shape"
      ],
      "metadata": {
        "id": "J5TN3HZS2fju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab90245a-bd10-4fc4-8f6b-c02de687cee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 128, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HOG Feature Extraction\n",
        "\n",
        "✅ Calculate Gradient\n",
        "\n",
        "✅ Calculate Magnitude and Orientation\n",
        "\n",
        "✅ Calculate HOG in 8*8 cells in 9 bins\n",
        "\n",
        "✅ Normalize gradients in 8x8 cells (2x2 cells of 8*8 block)\n",
        "\n",
        "Reference code for a single pixel HOG extraction was sourced and adjusted to fit an entire image and list of images.\n",
        "\n",
        "Comparing with the high-level off-shelf functions used in second section below, using a 3 by 3 cell size is seen to produce a better result than a 2*2 cell size."
      ],
      "metadata": {
        "id": "06fqv0OT0XGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate Gradient**\n",
        "\n",
        "Sobel operator is used to compute the gradient along x and y axis.\n",
        "\n"
      ],
      "metadata": {
        "id": "94o1awWH2up9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import io, color\n",
        "\n",
        "def extractGradient(img):\n",
        "\n",
        "  sobel_x = np.array([[-1, 0, 1],[-2, 0, 2],[-1, 0, 1]])\n",
        "  sobel_y = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
        "\n",
        "  G_x = sig.convolve2d(img, sobel_x, mode='same') \n",
        "  G_y = sig.convolve2d(img, sobel_y, mode='same') \n",
        "\n",
        "  return (G_x, G_y)"
      ],
      "metadata": {
        "id": "G4KB8NRk5Ud3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract features**\n",
        "\n",
        "**calculate_magnitude_hist_cell** is used to calculate the magnitude and orientation of 8*8 pixel. The magnitude is distributed between two bin values closest to it.\n",
        "\n",
        "**distribute_bucket_vals** is used to determine which two bins the magnitude should occupy based on the orientation and how the magnitude will be distributed between the two bins.\n",
        "\n",
        "**calculate_magnitude_hist_block** is used to normalize the gradients"
      ],
      "metadata": {
        "id": "_Y0ocQiX4DLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "N_BUCKETS = 9\n",
        "CELL_SIZE = 8  # 8x8 pixels\n",
        "BLOCK_SIZE = 2  # 2x2 cells\n",
        "\n",
        "def distribute_bucket_vals(m, d, bucket_vals):\n",
        "    left_bin = int(d / 20.)\n",
        "    # Also checks cases where the direction is between [160, 180)\n",
        "    right_bin = (int(d / 20.) + 1) % N_BUCKETS\n",
        "    assert 0 <= left_bin < right_bin < N_BUCKETS\n",
        "\n",
        "    left_val= m * (right_bin * 20 - d) / 20\n",
        "    right_val = m * (d - left_bin * 20) / 20\n",
        "    bucket_vals[left_bin] += left_val\n",
        "    bucket_vals[right_bin] += right_val\n",
        "    \n",
        "\n",
        "def calculate_magnitude_hist_cell(loc__x, loc__y, gradient, eps=1e-5):\n",
        "    # (loc__x, loc__y) defines the top left corner of the target cell.\n",
        "    cell__x = gradient[0][loc__x:loc__x + CELL_SIZE, loc__y:loc__y + CELL_SIZE]\n",
        "    cell__y = gradient[1][loc__x:loc__x + CELL_SIZE, loc__y:loc__y + CELL_SIZE]\n",
        "    magnitudes = np.sqrt(cell__x * cell__x + cell__y * cell__y)\n",
        "    # np.seterr(divide='ignore', invalid='ignore')\n",
        "    directions = np.abs(np.arctan(cell__y / (cell__x + eps)) * 180 / np.pi)\n",
        "\n",
        "    buckets = np.linspace(0, 180, N_BUCKETS + 1)\n",
        "    bucket_vals = np.zeros(N_BUCKETS)\n",
        "\n",
        "    list(map(\n",
        "        lambda m: distribute_bucket_vals(m[0], m[1], bucket_vals), \n",
        "        zip(magnitudes.flatten(), directions.flatten())\n",
        "    ))\n",
        "    return bucket_vals\n",
        "\n",
        "# This is used for 2*2 block size\n",
        "def calculate_magnitude_hist_block(loc__x, loc__y, gradient):\n",
        "    # (loc__x, loc__y) defines the top left corner of the target block.\n",
        "    # this implements the 2 by 2 block size\n",
        "    return reduce(\n",
        "        lambda arr1, arr2: np.concatenate((arr1, arr2)),\n",
        "        [calculate_magnitude_hist_cell(x, y, gradient) for x, y in zip(\n",
        "            [loc__x, loc__x + CELL_SIZE, loc__x, loc__x + CELL_SIZE],\n",
        "            [loc__y, loc__y, loc__y + CELL_SIZE, loc__y + CELL_SIZE],\n",
        "        )]\n",
        "    )\n",
        "# This is used for 3*3 block size\n",
        "# def calculate_magnitude_hist_block(loc__x, loc__y, gradient):\n",
        "#     # (loc__x, loc__y) defines the top left corner of the target block.\n",
        "#     # this implements the 2 by 2 block size\n",
        "#     return reduce(\n",
        "#         lambda arr1, arr2: np.concatenate((arr1, arr2)),\n",
        "#         [calculate_magnitude_hist_cell(x, y, gradient) for x, y in zip(\n",
        "#             [loc__x, loc__x + CELL_SIZE, loc__x + (2*CELL_SIZE), loc__x, loc__x + CELL_SIZE, loc__x + (2*CELL_SIZE), loc__x, loc__x + CELL_SIZE, loc__x + (2*CELL_SIZE)],\n",
        "#             [loc__y, loc__y, loc__y, loc__y + CELL_SIZE, loc__y + CELL_SIZE, loc__y + CELL_SIZE, loc__y + (2*CELL_SIZE), loc__y + (2*CELL_SIZE), loc__y + (2*CELL_SIZE)],\n",
        "#         )]\n",
        "#     )"
      ],
      "metadata": {
        "id": "OKlZKBDaKkEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section is a function that loops through all patches(positve and negative) to extract HOG features. This function calls both the extractGradient and Get_magnitude_hist_block functions and returns an array of dataset of image features."
      ],
      "metadata": {
        "id": "YGkJncV176nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all(image):\n",
        "\n",
        "  ydata_array = []\n",
        "  xdata_array = []\n",
        "  bucket_names_array = []\n",
        "\n",
        "  gradient = extractGradient(image)\n",
        "\n",
        "  img = image\n",
        "\n",
        "\n",
        "  for x in range(0, img.shape[0]):\n",
        "    for y in range(0, img.shape[1]):\n",
        "      ydata = calculate_magnitude_hist_block(x, y, gradient)\n",
        "      ydata = ydata / np.linalg.norm(ydata)\n",
        "\n",
        "      xdata = range(len(ydata))\n",
        "      bucket_names = np.tile(np.arange(N_BUCKETS), BLOCK_SIZE * BLOCK_SIZE)\n",
        "\n",
        "      ydata_array.append(ydata)\n",
        "      xdata_array.append(xdata)\n",
        "      bucket_names_array.append(bucket_names)\n",
        "\n",
        "      # This commented part plots a histogram showing the distirbution of bin values in the 4 blocks(2*2)\n",
        "      # plt.figure(figsize=(10, 3))\n",
        "      # plt.bar(xdata, ydata, align='center', alpha=0.8, width=0.9)\n",
        "      # plt.xticks(xdata, bucket_names * 20, rotation=90)\n",
        "      # plt.grid(ls='--', color='k', alpha=0.1)\n",
        "      # plt.tight_layout()\n",
        "\n",
        "  ydata_array = reduce(\n",
        "    lambda arr1, arr2: np.concatenate((arr1,arr2)),\n",
        "    ydata_array\n",
        "\n",
        "  )\n",
        "  return ydata_array"
      ],
      "metadata": {
        "id": "wThVAgerSEQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section calls the above function and saves the result in X. Not all positive and negative patches were used here in other to reduce run-time has the function as a high run-time"
      ],
      "metadata": {
        "id": "RDFnbdi88h4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n",
        "\n",
        "X = np.array([extract_all(image) for image in chain(positive_patches, negative_patches)])"
      ],
      "metadata": {
        "id": "eKMKDBRRHcVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.zeros(X.shape[0])\n",
        "y[0:positive_patches.shape[0]] = 1"
      ],
      "metadata": {
        "id": "xdaO17vzusUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "This section handles splitting, training and testing of the dataset"
      ],
      "metadata": {
        "id": "jwlD5umw9Dkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The X dataset and y labels are split into training data and test data"
      ],
      "metadata": {
        "id": "mWbudKO69RkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=10)"
      ],
      "metadata": {
        "id": "jtcmZztk8JXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVC is then used to create a classifier using the training dataset."
      ],
      "metadata": {
        "id": "LG5rc1lS9XPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "human_detection = SVC()\n",
        "human_detection.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "8lWoeFja8Vsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_detection.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dITBSiIy8lrG",
        "outputId": "32858067-0f44-49cc-c61f-49d329634730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human Detection\n",
        "Here an image containing human and other objects is used to evaluate the system."
      ],
      "metadata": {
        "id": "4QppXW0F9e7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage\n",
        "from skimage import io, color\n",
        "from skimage.io import imread_collection\n",
        "\n",
        "\n",
        "load_pattern = '/content/drive/MyDrive/Colab Notebooks/street_video assessed_test /*.png'\n",
        "frames=imread_collection(load_pattern)\n",
        "\n",
        "skimage.io.imshow(frames[0])\n",
        "\n",
        "test_image = frames[0]\n",
        "\n",
        "test_image_gray_0 = cv2.cvtColor(test_image, cv2.COLOR_RGB2GRAY)\n"
      ],
      "metadata": {
        "id": "bNSdzukwBM2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(test_image_gray_0)"
      ],
      "metadata": {
        "id": "7e85TZfTCMNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_window(image, stepSize, windowSize):\n",
        "  for y in range(0, image.shape[0]-windowSize[0], stepSize):\n",
        "    for x in range(0, image.shape[1]-windowSize[1], stepSize):\n",
        "      patch = image[y:y+windowSize[0],x:x+windowSize[1]]\n",
        "      yield(x,y),patch"
      ],
      "metadata": {
        "id": "d-AqOmwbCOQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices,patches = zip(*sliding_window(test_image_gray_0,10,positive_patches[0].shape))"
      ],
      "metadata": {
        "id": "WBrHVKQmDQv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features of the new image are extracted"
      ],
      "metadata": {
        "id": "RM2y2oIW9tlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hog_2 = np.array([extract_all(patch) for patch in patches])\n"
      ],
      "metadata": {
        "id": "-6638sxZhB1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trained dataset is now used to label which parts of the image contains humans"
      ],
      "metadata": {
        "id": "rRn1zGDc9yLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = human_detection.predict(hog_2)\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(12,20))\n",
        "ax.imshow(test_image_gray_0[190:350, :])\n",
        "ax.axis('off')\n",
        "\n",
        "Ni, Nj = positive_patches[0].shape\n",
        "indices =np.array(indices)\n",
        "\n",
        "track = []\n",
        "\n",
        "for i, j in indices[labels == 1]:\n",
        "    ax.add_patch(plt.Rectangle((i, j), Nj, Ni, edgecolor='red',alpha=0.3, lw=2, facecolor='none')) \n",
        "    track.append([i,j])  \n"
      ],
      "metadata": {
        "id": "H2RLXRXiETWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tracking\n",
        "\n",
        "Here, human frames from the human detection image are tracked."
      ],
      "metadata": {
        "id": "SY6wplV-NzHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the last code for Human detection, the index positions of the detected humans are saved to be used for tracking."
      ],
      "metadata": {
        "id": "uf0z0IIhNzHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexes of detected humans used for bounding box"
      ],
      "metadata": {
        "id": "IKpaPuQmNzHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "track"
      ],
      "metadata": {
        "id": "IXOacbq7NzHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSRT tracking algorithm is used here.\n",
        "\n",
        ">the box location will be updated later with new frame"
      ],
      "metadata": {
        "id": "vjqoBsLLNzHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = cv2.TrackerCSRT_create()\n",
        "init_box = [210,170,64,128]\n",
        "\n",
        "tracker.init(frames[0], init_box)"
      ],
      "metadata": {
        "id": "_3QFh4D_NzHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "n_frames = len(frames) - 1\n",
        "boxes = np.zeros((n_frames,4), dtype='int')\n",
        "\n",
        "for i in range(n_frames):\n",
        "  ok, box = tracker.update(frames[i+1])\n",
        "  if ok:\n",
        "    boxes[i] = box"
      ],
      "metadata": {
        "id": "9AbwlowuNzHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y,w,h = init_box\n",
        "vis_image = cv2.rectangle(frames[0],(x,y),(x+w,y+h),(255,0,0),2)\n",
        "skimage.io.imshow(vis_image)"
      ],
      "metadata": {
        "id": "rkA9Il8-NzHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_output_file = '/content/drive/MyDrive/Colab Notebooks/test2.avi'\n",
        "cc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "writer = cv2.VideoWriter(video_output_file,cc,30,(vis_image.shape[1],vis_image.shape[0]),True)\n",
        "\n",
        "writer.write(vis_image[:,:,::-1]) #BGR-> RGB\n",
        "\n",
        "for i,(x,y,w,h) in enumerate(boxes):\n",
        "  if x!=0:\n",
        "    vis_image = cv2.rectangle(frames[i],(x,y),(x+w,y+h),(255,0,0),2)\n",
        "    writer.write(vis_image[:,:,::-1])\n",
        "\n",
        "writer.release()"
      ],
      "metadata": {
        "id": "hsxV6TlyNzHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "The confusion matrix for the dataset is calculated"
      ],
      "metadata": {
        "id": "bey-zNP-hz7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, human_detection.predict(X_test))"
      ],
      "metadata": {
        "id": "6IHXpVG7eAN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sb\n",
        "plt.figure(figsize=(10,7))\n",
        "sb.heatmap(cm, annot=True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "id": "-UpaE1mygR_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Human detection using high-level off-shelf libraries\n",
        "This code-base uses high-level off-shelf libraries to detect humans in a video recording."
      ],
      "metadata": {
        "id": "VT3DZRcm3GTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kUWwm1Z3T7o",
        "outputId": "e88f4a47-d176-4a8a-e2b4-1f502a24d53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "LorcjHia3eG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset preparation\n",
        "dataset preparation"
      ],
      "metadata": {
        "id": "xJQR7Kbp3mtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative and Positive patches creation\n",
        "\n",
        "Here the negative and positive patches are imported and created respectively and preprocessing is done on both dataset. The positive patches are first created and the negative patches are created using the shape of positive patches because the shape and format have to be the same."
      ],
      "metadata": {
        "id": "XXwOQiBD3syM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image set containing just humans are imported and converted to 2-D 128*64 image patches."
      ],
      "metadata": {
        "id": "2-N78lpa3yE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = glob.glob('/content/drive/MyDrive/Colab Notebooks/pedestrians128x64/*.ppm')\n",
        "\n",
        "positive_image_set = [] \n",
        "for item in path:\n",
        "  positive_image = cv2.imread(item)\n",
        "  postive_test = np.array(positive_image)\n",
        "  to_gray2 = cv2.cvtColor(postive_test, cv2.COLOR_BGR2GRAY)\n",
        "  positive_image_set.append(to_gray2)\n",
        "  \n",
        "positive_patches = np.array(positive_image_set)"
      ],
      "metadata": {
        "id": "WmuA8BZl3y51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative patches for were created from skimage exisitng image data and are reshaped and resized to into 2-D 128*64 image"
      ],
      "metadata": {
        "id": "4nWryft-36kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import data, color\n",
        "\n",
        "imgs_to_use = ['immunohistochemistry', 'hubble_deep_field']\n",
        "images = [color.rgb2gray(getattr(data, name)()) for name in imgs_to_use]"
      ],
      "metadata": {
        "id": "EYZ_oIzs4Cbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.image import PatchExtractor\n",
        "from skimage import transform\n",
        "import numpy as np\n",
        "\n",
        "def extract_patches(img, N, scale=1, patch_size=positive_patches[0].shape):\n",
        "    extracted_patch_size = tuple((scale * np.array(patch_size)).astype(int))\n",
        "    extractor = PatchExtractor(patch_size=extracted_patch_size,\n",
        "                               max_patches=N, random_state=0)\n",
        "    patches = extractor.transform(img[np.newaxis])\n",
        "    if scale != 1:\n",
        "        patches = np.array([transform.resize(patch, patch_size)\n",
        "                            for patch in patches])\n",
        "    return patches\n",
        "\n",
        "negative_patches = np.vstack([extract_patches(im, 1000, scale)\n",
        "                              for im in images for scale in [0.5, 1.0, 2.0]])"
      ],
      "metadata": {
        "id": "AJQBvnb_4EFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction\n",
        "\n",
        "This section includes the extraction of features using sklearn feature"
      ],
      "metadata": {
        "id": "uHvjInB84KRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "X which is an array of the features extracted from the images is gotten and the y value is retrieved using the first value for the shape of X which is vasically the number of labels from the dataset. The size(shape) of both X and Y have to be the same."
      ],
      "metadata": {
        "id": "xRScBuVQ4NV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After series of tests, cell size(cells_per_block) of (3,3) was used as it produced better result"
      ],
      "metadata": {
        "id": "Xeditmy04ZKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import feature\n",
        "from itertools import chain\n",
        "\n",
        "test_images = []\n",
        "fds = []\n",
        "\n",
        "X = np.array([feature.hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
        "                    cells_per_block=(3, 3) ) for image in chain(positive_patches, negative_patches)])\n",
        "y = np.zeros(X.shape[0])\n",
        "y[0:positive_patches.shape[0]] = 1\n"
      ],
      "metadata": {
        "id": "2RNL9qh64aQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "This section handles splitting, training and testing of the dataset"
      ],
      "metadata": {
        "id": "tB3uLbSV4k8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The X dataset and y labels are split into training data and test data"
      ],
      "metadata": {
        "id": "U-WaIK3C4omq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=4)"
      ],
      "metadata": {
        "id": "kHUd4aBq4tp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVC is then used to create a classifier using the training dataset."
      ],
      "metadata": {
        "id": "uVY58qg44xJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "human_detection = SVC()\n",
        "human_detection.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5zy4yxJ41MS",
        "outputId": "16e4341e-dab2-43e7-8a40-7fd2f76cc3a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_detection.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6MA50BS449K",
        "outputId": "257d551d-1d44-48ab-d128-a35c41d80de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9994222992489891"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human Detection\n",
        "Here an image containing human and other objects is used to evaluate the system."
      ],
      "metadata": {
        "id": "OMfdJoDY49A4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import io, color\n",
        "import skimage\n",
        "from skimage.io import imread_collection\n",
        "\n",
        "load_pattern = '/content/drive/MyDrive/Colab Notebooks/street_video assessed_test /*.png'\n",
        "frames=imread_collection(load_pattern)\n",
        "\n",
        "skimage.io.imshow(frames[0])\n",
        "\n",
        "test_image = frames[0]\n",
        "\n",
        "test_image_gray = cv2.cvtColor(test_image, cv2.COLOR_RGB2GRAY)\n"
      ],
      "metadata": {
        "id": "usR_2gsr5CZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sliding window is used to scan the image while also creating patches of the same size the training dataset from the new imported image"
      ],
      "metadata": {
        "id": "DZLpX_Oh5ORx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_window(image, stepSize, windowSize):\n",
        "  for y in range(0, image.shape[0]-windowSize[0], stepSize):\n",
        "    for x in range(0, image.shape[1]-windowSize[1], stepSize):\n",
        "      patch = image[y:y+windowSize[0],x:x+windowSize[1]]\n",
        "      yield(x,y),patch"
      ],
      "metadata": {
        "id": "FHxkNIUq5UpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices,patches = zip(*sliding_window(test_image_gray,10,positive_patches[0].shape))"
      ],
      "metadata": {
        "id": "oRV_-SXF5WuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features of the new image are extracted"
      ],
      "metadata": {
        "id": "v7Ox6Yxz5b9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "picture_hog_2 = np.array([feature.hog(patch, orientations=9, pixels_per_cell=(8, 8),\n",
        "                    cells_per_block=(3, 3)) for patch in patches])"
      ],
      "metadata": {
        "id": "xSlc-gr05mbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trained dataset is now used to label which parts of the image contains humans"
      ],
      "metadata": {
        "id": "2Rdg7Kvi5qAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "labels = human_detection.predict(picture_hog_2)\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(12,20))\n",
        "ax.imshow(test_image)\n",
        "ax.axis('off')\n",
        "\n",
        "Ni, Nj = positive_patches[0].shape\n",
        "indices =np.array(indices)\n",
        "\n",
        "track = []\n",
        "\n",
        "\n",
        "for i, j in indices[labels == 1]:\n",
        "    #ax.add_patch(plt.Rectangle((i,j), 30, 45,  edgecolor='red',alpha=0.3, lw=2, facecolor='none'))  \n",
        "    ax.add_patch(plt.Rectangle((i, j), Nj, Ni, edgecolor='red',alpha=0.3, lw=2, facecolor='none'))\n",
        "    track.append([i,j])  \n"
      ],
      "metadata": {
        "id": "pdE9GiF85uRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tracking\n",
        "\n",
        "Here, human frames from the human detection image are tracked."
      ],
      "metadata": {
        "id": "5DMSfv_h5yDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the last code for Human detection, the index positions of the detected humans are saved to be used for tracking."
      ],
      "metadata": {
        "id": "8-aqnXAt51s2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexes of detected humans used for bounding box"
      ],
      "metadata": {
        "id": "wQa4nkp1523v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "track"
      ],
      "metadata": {
        "id": "uJi5ZRis57Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSRT tracking algorithm is used here.\n",
        "\n",
        ">the box location will be updated later with new frame"
      ],
      "metadata": {
        "id": "UP6DoZZY6FVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = cv2.TrackerCSRT_create()\n",
        "init_box = [210,170,64,128]\n",
        "\n",
        "tracker.init(frames[0], init_box)"
      ],
      "metadata": {
        "id": "2n8gkbfo6JZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "n_frames = len(frames) - 1\n",
        "boxes = np.zeros((n_frames,4), dtype='int')\n",
        "\n",
        "for i in range(n_frames):\n",
        "  ok, box = tracker.update(frames[i+1])\n",
        "  if ok:\n",
        "    boxes[i] = box"
      ],
      "metadata": {
        "id": "GuglArCm6RkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y,w,h = init_box\n",
        "vis_image = cv2.rectangle(frames[0],(x,y),(x+w,y+h),(255,0,0),2)\n",
        "skimage.io.imshow(vis_image)"
      ],
      "metadata": {
        "id": "EeGzsfRA6zay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_output_file = '/content/drive/MyDrive/Colab Notebooks/test2.avi'\n",
        "cc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "writer = cv2.VideoWriter(video_output_file,cc,30,(vis_image.shape[1],vis_image.shape[0]),True)\n",
        "\n",
        "writer.write(vis_image[:,:,::-1]) #BGR-> RGB\n",
        "\n",
        "for i,(x,y,w,h) in enumerate(boxes):\n",
        "  if x!=0:\n",
        "    vis_image = cv2.rectangle(frames[i],(x,y),(x+w,y+h),(255,0,0),2)\n",
        "    writer.write(vis_image[:,:,::-1])\n",
        "\n",
        "writer.release()"
      ],
      "metadata": {
        "id": "KdcZXcqr7DIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix\n",
        "\n",
        "The confusion matrix for the dataset is calculated"
      ],
      "metadata": {
        "id": "lfwph2C9kMMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, human_detection.predict(X_test))"
      ],
      "metadata": {
        "id": "-Nhh42JTkMMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sb\n",
        "plt.figure(figsize=(10,7))\n",
        "sb.heatmap(cm, annot=True)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "id": "iMTuk0CzkMMu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}